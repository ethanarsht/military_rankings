{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scrape battles from Wikipedia (Archival)\n",
    "\n",
    "The original system scraped Wikipedia closely to the following. I've edited it to get rid of the `global` statements and remove side effects from the functions so that they can be used with `threading` or `multiprocessing` (and then do so for the longer scrape)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports and config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "from multiprocessing.pool import Pool\n",
    "\n",
    "import requests\n",
    "import pandas as pd\n",
    "\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.options.display.max_rows = 50\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scrape list pages to get battle page URLs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### List pages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table_pages = ['https://en.wikipedia.org/wiki/List_of_battles_before_301',\n",
    "               'https://en.wikipedia.org/wiki/List_of_battles_301%E2%80%931300',\n",
    "               'https://en.wikipedia.org/wiki/List_of_battles_1301%E2%80%931600',\n",
    "               'https://en.wikipedia.org/wiki/List_of_battles_since_2001']\n",
    "\n",
    "list_pages = ['https://en.wikipedia.org/wiki/List_of_battles_1601%E2%80%931800',\n",
    "               'https://en.wikipedia.org/wiki/List_of_battles_1801%E2%80%931900',\n",
    "               'https://en.wikipedia.org/wiki/List_of_battles_1901%E2%80%932000',]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scrape functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_battles(url):\n",
    "    link_list = []\n",
    "    title_list = []\n",
    "    \n",
    "    with requests.Session() as session:\n",
    "        response = session.get(url)\n",
    "        \n",
    "    soup = BeautifulSoup(response.content)\n",
    "    bullets = soup.find_all('li')\n",
    "    \n",
    "    for bullet in bullets:\n",
    "        if 'title' in str(bullet):\n",
    "            if 'footer' not in str(bullet):\n",
    "                link = bullet.a['href']\n",
    "                link_list.append(link)\n",
    "                \n",
    "                title = bullet.a.get('title', None)\n",
    "                title_list.append(title)\n",
    "\n",
    "    return link_list, title_list\n",
    "\n",
    "# Them's\n",
    "fightin_words = ['Fall', 'Battle', 'Siege', 'Capture', 'Operation', 'Action', 'Recapture']\n",
    "\n",
    "def retrieve_early_battles(url):\n",
    "    link_list = []\n",
    "    title_list = []\n",
    "    \n",
    "    with requests.Session() as session:\n",
    "        responses = requests.get(url)\n",
    "        soup = BeautifulSoup(responses.content)\n",
    "        tables = soup.find_all('table', {'class': 'wikitable'})\n",
    "        \n",
    "        for table in tables:\n",
    "            cells = table.find_all('td')\n",
    "            for cell in cells:\n",
    "                if cell.a is not None:\n",
    "                    if any(x in str(cell.a['href']) for x in fightin_words):\n",
    "                        title_list.append(cell.a['title'])\n",
    "                        link_list.append(cell.a['href'])\n",
    "                        \n",
    "    return link_list, title_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run scrape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "link_list = []\n",
    "title_list = []\n",
    "\n",
    "for url in list_pages:\n",
    "    links, titles = retrieve_battles(url)\n",
    "    link_list += links\n",
    "    title_list += titles\n",
    "\n",
    "late_battles = pd.DataFrame.from_dict({'url': link_list, 'title': title_list})\n",
    "late_battles.drop_duplicates(inplace=True)\n",
    "late_battles.dropna(subset=['title'], inplace=True)\n",
    "\n",
    "print(\"# Late battles:\", len(late_battles))\n",
    "late_battles.sample(n=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "link_list = []\n",
    "title_list = []\n",
    "\n",
    "for url in table_pages:\n",
    "    links, titles = retrieve_early_battles(url)\n",
    "    link_list += links\n",
    "    title_list += titles\n",
    "\n",
    "early_battles = pd.DataFrame.from_dict({'url': link_list, 'title': title_list})\n",
    "early_battles.drop_duplicates(inplace=True)\n",
    "early_battles.dropna(subset=['title'], inplace=True)\n",
    "\n",
    "print(\"# Early battles:\", len(early_battles))\n",
    "early_battles.sample(n=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combine battles and save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "battles = pd.concat([early_battles, late_battles])\n",
    "battles.reset_index(drop=True, inplace=True)\n",
    "battles.drop_duplicates(inplace=True)\n",
    "\n",
    "print('Total # battles:', len(battles))\n",
    "battles.sample(n=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "battles.to_csv('./data/all_battles.tsv', encoding='utf-8', sep='\\t')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load saved dataframes\n",
    "\n",
    "(Resume work)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "battles = pd.read_csv('./data/all_battles.tsv', encoding='utf-8', sep='\\t', index_col=0)\n",
    "# Exclude nonexistent pages or non-english wikipedia pages\n",
    "# (There aren't that many, so better to dump them)\n",
    "battles = battles.loc[~battles.title.str.contains('does not exist')]\n",
    "battles = battles.loc[~battles.url.str.contains('https')]\n",
    "\n",
    "# Exclude wars\n",
    "battles = battles.loc[~battles.title.map(lambda s: 'War' in s.split(' '))]\n",
    "\n",
    "# Exclude support page links etc\n",
    "battles = battles.loc[~battles.url.str.contains('Wikipedia')]\n",
    "battles = battles.loc[~battles.url.str.contains('index.php')]\n",
    "battles = battles.loc[~battles.url.str.contains('Special:')]\n",
    "battles = battles.loc[~battles.url.str.contains('List_of')]\n",
    "battles = battles.loc[~battles.url.str.contains('Category:')]\n",
    "\n",
    "# We only want battles, not extended campaigns or distributed multi-event things\n",
    "conflict_terms = ('battle', 'siege', 'fall', 'sack', \n",
    "                  'operation', 'capture', 'raid', \n",
    "                  'action', 'destruction', 'massacre')\n",
    "\n",
    "battles = battles.loc[battles.title.map(lambda s: any(b in s.lower() for b in conflict_terms))]\n",
    "len(battles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "omit_list = ['Capital punishment',\n",
    "             'Military advisor',\n",
    "             'Wounded in action',\n",
    "             'Prisoner of war',\n",
    "             'Killed in action',\n",
    "             'Surrender (military)',\n",
    "             'Surrendered',\n",
    "             'Common military ranks in English']\n",
    "\n",
    "redirect_names = ['Napoleon I', 'Alexander III of Macedon']\n",
    "\n",
    "non_english_tokens = ['pt.', 'tr.', 'ko.', 'ja.',\n",
    "                      'th.', 'da.', 'es.', 'de.',\n",
    "                      'it.', 'fr.', 'zh.']\n",
    "\n",
    "\n",
    "def get_full_url(url):\n",
    "    if 'https://' in url:\n",
    "        return url\n",
    "    else:\n",
    "        return f'https://en.wikipedia.org{url}'\n",
    "\n",
    "\n",
    "def table_scrape(url, name):\n",
    "    battle_page_url = get_full_url(url)\n",
    "    try:\n",
    "        battle_page_response = requests.get(battle_page_url)\n",
    "    except requests.ConnectionError:\n",
    "        print(f'Initial request error for {battle_page_url}')\n",
    "        return None\n",
    "\n",
    "    battle_page_soup = BeautifulSoup(battle_page_response.text)\n",
    "    infobox = battle_page_soup.find('table', {'class': 'infobox vevent'})\n",
    "\n",
    "    if infobox is not None:\n",
    "        try:\n",
    "            details, stub = pd.read_html(str(infobox), )\n",
    "        except ValueError:\n",
    "            print(f\"Error parsing details table from HTML from {battle_page_url}\")\n",
    "            return str(infobox)\n",
    "    else:\n",
    "        print(f\"No infobox found for {battle_page_url}\")\n",
    "        return None\n",
    "\n",
    "    # TODO: also keep belligerents info, not just leader\n",
    "    if 'Belligerents' in str(infobox):\n",
    "        try:\n",
    "            clean_rows = stub.drop_duplicates(0).set_index(0, drop=True).loc[['Date', 'Location', 'Result'], :]\n",
    "        except KeyError:\n",
    "            print(f'Error extracting rows from table from {battle_page_url}')\n",
    "            return None\n",
    "\n",
    "        clean_col = clean_rows.transpose()\n",
    "        clean_col.reset_index(drop=True, inplace=True)\n",
    "\n",
    "        html_table = infobox.find_all('tr')\n",
    "        belligerents = defaultdict(list)\n",
    "\n",
    "        lhs_df = pd.DataFrame()\n",
    "        rhs_df = pd.DataFrame()\n",
    "\n",
    "        for index, cell in enumerate(html_table):\n",
    "            # Parse commanders and leaders cell contents\n",
    "            if 'Commanders and leaders' in str(cell):\n",
    "                leaders = html_table[index + 1]\n",
    "                cells = leaders.find_all('td')\n",
    "\n",
    "                for side, cell in enumerate(cells):\n",
    "                    anchors = cell.find_all('a')\n",
    "\n",
    "                    for anchor in anchors:\n",
    "                        anchor_string = str(anchor)\n",
    "\n",
    "                        if 'title' in anchor_string:\n",
    "                            # If any of this text is in anchor string,\n",
    "                            # it's not a link we want to keep - move on\n",
    "                            bad_text = ('class=\"image\"', 'class=\"thumbborder\"',\n",
    "                                        'cite_note', 'disambiguation needed',\n",
    "                                        'cnote_g')\n",
    "\n",
    "                            if any(map(lambda s: s in anchor_string, bad_text)):\n",
    "                                continue\n",
    "\n",
    "                            title = anchor['title']\n",
    "                            if title in redirect_names:\n",
    "                                print(f\"Redirect for {title}\")\n",
    "\n",
    "                            href = anchor['href']\n",
    "                            leader_page_url = get_full_url(href)\n",
    "                            ul = battle_page_soup.find('ul', {'class': 'redirectText'})\n",
    "\n",
    "                            if len(href) <= 10:\n",
    "                                pass\n",
    "                            elif href[8] + href[9] + href[10] in non_english_tokens:\n",
    "                                print(f'Non-english page found at {leader_page_url}')\n",
    "                            elif ul is not None:\n",
    "                                print(\n",
    "                                    f'Redirected for {title} at {leader_page_url}')\n",
    "                                leader_page_url = get_full_url(ul.a['href'])\n",
    "\n",
    "                            try:\n",
    "                                leader_response = requests.get(leader_page_url)\n",
    "                            except requests.ConnectionError:\n",
    "                                print(f\"Leader request error for {leader_page_url}\")\n",
    "                                return None\n",
    "\n",
    "                            leader_soup = BeautifulSoup(leader_response.text)\n",
    "                            leader_name = leader_soup.find('title')\n",
    "\n",
    "                            # Blank leader? Move on.\n",
    "                            if not leader_name:\n",
    "                                continue\n",
    "                                \n",
    "                            leader_name = leader_name.text.replace(' - Wikipedia', '')\n",
    "\n",
    "                            # These aren't actually links to leaders - move on\n",
    "                            if leader_name in omit_list:\n",
    "                                continue\n",
    "\n",
    "                            belligerents[side].append(leader_name)\n",
    "\n",
    "        for leader in belligerents[0]:\n",
    "            clean_col['leader'] = leader\n",
    "            lhs_df = pd.concat([lhs_df, clean_col])\n",
    "\n",
    "        for leader in belligerents[1]:\n",
    "            clean_col['leader'] = leader\n",
    "            rhs_df = pd.concat([rhs_df, clean_col])\n",
    "\n",
    "        lhs_df.reset_index(drop=True, inplace=True)\n",
    "        rhs_df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "        # rhs_df = rhs_df.rename(columns={0: 'Location'})\n",
    "\n",
    "        if 'Strength' in str(details):\n",
    "            strength_finder = details.loc[details.iloc[:, 0] == 'Strength']\n",
    "            strength_row = strength_finder.index + 1\n",
    "            strength = details.loc[strength_row]\n",
    "            strength_x = strength.iloc[0, 0]\n",
    "            strength_y = strength.iloc[0, 1]\n",
    "        else:\n",
    "            strength_x = None\n",
    "            strength_y = None\n",
    "\n",
    "        lhs_df['own'] = strength_x\n",
    "        lhs_df['opp'] = strength_y\n",
    "\n",
    "        rhs_df['own'] = strength_y\n",
    "        rhs_df['opp'] = strength_x\n",
    "\n",
    "        if 'Casualties and losses' not in str(details):\n",
    "            lhs_df['taken'] = None\n",
    "            lhs_df['inflicted'] = None\n",
    "            rhs_df['taken'] = None\n",
    "            rhs_df['inflicted'] = None\n",
    "\n",
    "        else:\n",
    "            casualties_finder = details[details.iloc[:, 0] == 'Casualties and losses']\n",
    "            casualties_row = casualties_finder.index + 1\n",
    "            casualties = details.loc[casualties_row]\n",
    "            casualties_x = casualties.iloc[0, 0]\n",
    "            casualties_y = casualties.iloc[0, 1]\n",
    "\n",
    "            lhs_df['taken'] = casualties_x\n",
    "            lhs_df['inflicted'] = casualties_y\n",
    "            rhs_df['taken'] = casualties_y\n",
    "            rhs_df['inflicted'] = casualties_x\n",
    "\n",
    "        lhs_df['Battle'] = name\n",
    "        rhs_df['Battle'] = name\n",
    "\n",
    "        lhs_df['pos'] = 'L'\n",
    "        rhs_df['pos'] = 'R'\n",
    "\n",
    "        df = pd.concat([lhs_df, rhs_df])\n",
    "        df.reset_index(drop=True, inplace=True)\n",
    "        df.drop_duplicates(inplace=True)\n",
    "        return df\n",
    "\n",
    "    else:\n",
    "        print('Filtered: ' + name)\n",
    "        \n",
    "def map_func(row):\n",
    "    \"\"\"Run table_scrape with a single argument for use with itterrows.\"\"\"\n",
    "    return table_scrape(row.url, row.title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vroom\n",
    "# Using processes instead of threads as each function call combines IO/CPU blocking steps\n",
    "# If you don't have a fat CPU, use fewer processes\n",
    "\n",
    "with Pool(processes=32) as pool:\n",
    "    results = pool.map(map_func, map(lambda r: r[1], battles.iterrows()))\n",
    "    \n",
    "results = pd.concat([r for r in results if type(r) == type(pd.DataFrame())])\n",
    "results.reset_index(drop=True, inplace=True)\n",
    "results.drop_duplicates(inplace=True)\n",
    "results.sample(n=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Scraped page for {len(results.Battle.unique())} found battles / {len(battles)}\")\n",
    "print('Sample URLS')\n",
    "for url in battles.sample(n=5).url.map(lambda x: f\"https://en.wikipedia.org{x}\").tolist():\n",
    "    print(url)\n",
    "    \n",
    "results.leader.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results.to_csv('./data/scraped_battle_data.csv', encoding='utf-8')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "research",
   "language": "python",
   "name": "research"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
